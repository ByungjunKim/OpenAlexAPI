{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPHYKdsiPFXYh0a862iAN/w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ByungjunKim/OpenAlexAPI/blob/main/OpenAlex_Tokenization_TopicModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Collecting Bibliographic Data Using the OpenAlex API, Tokenizing, and Applying Topic Modeling"
      ],
      "metadata": {
        "id": "2fPc3aCPUP95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U -q requests natsort tqdm pyalex tomotopy gensim nltk 'spacy[cuda-autodetect]'"
      ],
      "metadata": {
        "id": "PruqkTyPGeSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# spacy 영어 모델 다운로드(다운로드 속도를 위해 sm(small) 모델 다운)\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "23icc0HmzY9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9DdvqSNsoD0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from tqdm.auto import tqdm\n",
        "tqdm.pandas()\n",
        "import json\n",
        "import glob\n",
        "import time\n",
        "import re\n",
        "from natsort import natsorted\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import pyalex\n",
        "from pyalex import Works, Authors, Sources, Institutions, Topics, Publishers, Funders\n",
        "\n",
        "import spacy\n",
        "print(spacy.prefer_gpu()) #GPU 활용\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "nlp.add_pipe('sentencizer')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from collections import Counter\n",
        "from itertools import chain\n",
        "\n",
        "import tomotopy as tp\n",
        "# print(tp.isa)\n",
        "import sys\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OpenAlex API (pyalex)"
      ],
      "metadata": {
        "id": "SEeQ7EJvzT_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pyalex.config.email = \"kuntakim88@gmail.com\" # insert your email"
      ],
      "metadata": {
        "id": "uLZcJ2TSMECz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_papers_by_title(title_keyword, start_year=None, end_year=None, test_mode=False):\n",
        "    # Set the number of results per page based on test_mode\n",
        "    per_page = 10 if test_mode else 200\n",
        "\n",
        "    # Create a search filter with title and publication year range if provided\n",
        "    search_filter = pyalex.Works().search_filter(title=title_keyword)\n",
        "    if start_year and end_year:\n",
        "        search_filter = search_filter.filter(from_publication_date=f\"{start_year}-01-01\").filter(to_publication_date=f\"{end_year}-12-31\")\n",
        "\n",
        "    # Create a paginator to get all works with the specified keyword and year range\n",
        "    pager = search_filter.paginate(per_page=per_page, n_max=None)\n",
        "\n",
        "    # Get the total number of works\n",
        "    total_works = search_filter.count()\n",
        "    print(f\"Total number of works related to '{title_keyword}' from {start_year} to {end_year}: {total_works}\")\n",
        "\n",
        "    papers = []\n",
        "\n",
        "    # Iterate through each page and collect all bibliographic information (limit to one page if test_mode is True)\n",
        "    for page in tqdm(pager, total=(total_works // per_page) + 1, desc=\"Collecting papers\"):\n",
        "        for work in page:\n",
        "            papers.append(work)\n",
        "        if test_mode:\n",
        "            break  # Only collect one page for testing purposes\n",
        "\n",
        "    # Print the total number of works collected\n",
        "    print(f\"Total number of works collected: {len(papers)}\")\n",
        "\n",
        "    return papers"
      ],
      "metadata": {
        "id": "op98qsfxEOBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage: fetch and display papers related to a specified keyword and year range\n",
        "results = fetch_papers_by_title(\"renewable energy\", start_year=2022, end_year=2024)"
      ],
      "metadata": {
        "id": "gvXVj09Allji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pandas dataframe으로 변환\n",
        "df = pd.DataFrame.from_dict(results)\n",
        "df"
      ],
      "metadata": {
        "id": "hBsegzVLK1tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop_duplicates(subset=['id']).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "Qy7nuoV1K3P2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "BeZafX_PwASI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def index_to_text(abstract_inverted_index):\n",
        "    abstract_index = {}\n",
        "    for k, vlist in abstract_inverted_index.items():\n",
        "        for v in vlist:\n",
        "            abstract_index[v] = k\n",
        "    abstract = ' '.join(abstract_index[k] for k in sorted(abstract_index.keys()))\n",
        "    return abstract"
      ],
      "metadata": {
        "id": "QPX2bw-1K6de"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# index to text\n",
        "df['abstract'] = df[~pd.isna(df['abstract_inverted_index'])]['abstract_inverted_index'].progress_map(lambda x:index_to_text(x))\n",
        "df['abstract']"
      ],
      "metadata": {
        "id": "UmnM5t-pK8xJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna(subset=['abstract']).reset_index(drop=True) # 초록이 없는 행 삭제"
      ],
      "metadata": {
        "id": "sW4sbwm1K9iW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df)"
      ],
      "metadata": {
        "id": "nSWhvWvkFNFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization with spaCy"
      ],
      "metadata": {
        "id": "OwIxRqwzLELJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://spacy.io/usage/linguistic-features\n",
        "#Lemmatization 처리된 토큰 추출 (https://wikidocs.net/21707)\n",
        "df['tokens'] = df['abstract'].progress_map(lambda x:[token.lemma_+'/'+token.pos_ for token in nlp(x)])\n",
        "# df['tokens'] = df['abstract'].progress_map(lambda x:[token.lemma_ for token in nlp(x)])\n",
        "df['tokens']"
      ],
      "metadata": {
        "id": "tjxsl2-Lz25I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "allowed_postags = ['ADJ','NOUN','VERB','PROPN','ADV'] # 추출하고 싶은 품사 리스트 (형용사, 명사, 동사, 고유명사, 부사)"
      ],
      "metadata": {
        "id": "LE_flBT4K_Ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['allowed_tokens'] = df['tokens'].map(lambda x:[token for token in x if token.split('/')[1] in allowed_postags])\n",
        "df['allowed_tokens']"
      ],
      "metadata": {
        "id": "p_I042suzxaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# top n Unigram\n",
        "unigram = chain(*df['allowed_tokens'])\n",
        "cnt = Counter(unigram)\n",
        "cnt.most_common(30) # Top N"
      ],
      "metadata": {
        "id": "6pkhIe1r2zWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = ['%/NOUN', 'paper/NOUN', 'research/NOUN', 'study/NOUN']"
      ],
      "metadata": {
        "id": "-LB0NyuF3D-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove stopwords\n",
        "df['allowed_tokens'] = df['allowed_tokens'].map(lambda x:[t for t in x if not t in stop_words])"
      ],
      "metadata": {
        "id": "5GlwQlqq3WDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# top n Unigram\n",
        "unigram = chain(*df['allowed_tokens'])\n",
        "cnt = Counter(unigram)\n",
        "cnt.most_common(30) # Top N"
      ],
      "metadata": {
        "id": "8-fgv6-P3XKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Topic Model with tomotopy"
      ],
      "metadata": {
        "id": "o9GRYa0h2uBp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['topics_name'] = df['topics'].dropna().progress_map(lambda x:[t['display_name'] for t in x])\n",
        "df['subfield_name'] = df['topics'].dropna().progress_map(lambda x:[t['subfield']['display_name'] for t in x])\n",
        "df['field_name'] = df['topics'].dropna().progress_map(lambda x:[t['field']['display_name'] for t in x])\n",
        "df['domain_name'] = df['topics'].dropna().progress_map(lambda x:[t['domain']['display_name'] for t in x])"
      ],
      "metadata": {
        "id": "xiuOR5UpLFdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[df['topics_name'].str.len()>0,'topics_name'].map(lambda x:x[0]).value_counts() # prime topic"
      ],
      "metadata": {
        "id": "_f52yyjiLLxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[df['field_name'].str.len()>0,'field_name'].map(lambda x:x[0]).value_counts()"
      ],
      "metadata": {
        "id": "FlGjHL0S11Yk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SMKhkvZ5aZJ"
      },
      "source": [
        "### 1. LDA\n",
        "https://bab2min.github.io/tomotopy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rhNLzOG5x2O"
      },
      "source": [
        "##### 모델 초기화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0bZwaTN5cXb"
      },
      "source": [
        "LDA = tp.LDAModel(k=10,min_df=10,tw=tp.TermWeight.PMI, rm_top=3, seed=2021) # Hyperparameter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7147G-Ia52in"
      },
      "source": [
        "##### 빈 모델에 토큰 리스트 넣어주기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9iRlDKI5kf2"
      },
      "source": [
        "for token in tqdm(df['allowed_tokens'].tolist()):\n",
        "    LDA.add_doc(token)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33pSfG1b6PVm"
      },
      "source": [
        "##### 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LDA.train(0)"
      ],
      "metadata": {
        "id": "vLNpHnmENBLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqbaOx906TmO"
      },
      "source": [
        "# 한번에 20회씩 총 500회 학습\n",
        "print('Num docs:', len(LDA.docs), ', Vocab size:', LDA.num_vocabs, ', Num words:', LDA.num_words)\n",
        "print('Removed top words:', LDA.removed_top_words)\n",
        "print('Training...', file=sys.stderr, flush=True)\n",
        "for i in range(0, 500, 20):\n",
        "    LDA.train(20)\n",
        "    print('Iteration: {}\\tLog-likelihood: {}'.format(i, LDA.ll_per_word))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ongHkC-M9N2O"
      },
      "source": [
        "# 학습 결과\n",
        "LDA.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "on2hEXTd6dba"
      },
      "source": [
        "##### 토픽별 TopN 단어 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-s6vXGjO6sW6"
      },
      "source": [
        "for i in range(LDA.k):\n",
        "    res = LDA.get_topic_words(i, top_n=10) # top 10\n",
        "    print('Topic #{}'.format(i), end='\\t')\n",
        "    print(', '.join(w for w, p in res))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5b7yoWt7h5q"
      },
      "source": [
        "##### 토픽 이름 자동으로 붙이기 (참고용)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7R12bmUK7mOE"
      },
      "source": [
        "# extract candidates for auto topic labeling\n",
        "extractor = tp.label.PMIExtractor(min_cf=10, min_df=10, max_len=5, max_cand=10000)\n",
        "cands = extractor.extract(LDA)\n",
        "\n",
        "labeler = tp.label.FoRelevance(LDA, cands, min_df=10, smoothing=1e-2, mu=0.25)\n",
        "for k in range(LDA.k):\n",
        "    print(\"== Topic #{} ==\".format(k))\n",
        "    print(\"Labels:\", ', '.join(label for label, score in labeler.get_topic_labels(k, top_n=5)))\n",
        "    for word, prob in LDA.get_topic_words(k, top_n=10):\n",
        "        print(word, prob, sep='\\t')\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !sudo apt-get update\n",
        "# !sudo apt-get install -y locales\n",
        "# !sudo locale-gen en_US.UTF-8\n",
        "# !sudo update-locale LANG=en_US.UTF-8\n",
        "# import locale\n",
        "# locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')\n",
        "# # Install tornado after setting the locale\n",
        "# !pip install tornado --upgrade"
      ],
      "metadata": {
        "id": "xgcIbXjPIrLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/bab2min/tomotopy/blob/0be609df83e606cc8c14240f5b552096f9435351/README.rst#interactive-model-viewer\n",
        "# tp.viewer.open_viewer(LDA, host=\"localhost\", port=9998)"
      ],
      "metadata": {
        "id": "B-cZlThTBFlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5INtHZoSsTxi"
      },
      "source": [
        "### 2. DTM (Dynamic Topic Model)\n",
        "\"Time series topic model\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eFENt8U8PJg"
      },
      "source": [
        "df['publication_year'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2i4I3jO8SDG"
      },
      "source": [
        "# DTM에서 t(시간 하이퍼하라미터) 형식으로 변환(0부터 시작)\n",
        "df['publication_year'] = df['publication_year'] - df['publication_year'].min()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HoUDcJZ8cki"
      },
      "source": [
        "# 0 : 2022, 1:2023, 2:2024\n",
        "df['publication_year'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ib4VlmrZ8dyd"
      },
      "source": [
        "##### 모델 초기화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBhL0d4c8f_1"
      },
      "source": [
        "# t = 3 이면 3개의 시기에 대한 DTM\n",
        "DTM = tp.DTModel(k=10,min_df=10,tw=tp.TermWeight.PMI,t=3,rm_top=3, seed=2021)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGJMHMsw8tfa"
      },
      "source": [
        "##### 빈 모델에 토큰 리스트와 시간변수 넣어주기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwUlPmU08rvz"
      },
      "source": [
        "token_year_dict = df[['allowed_tokens','publication_year']].to_dict('index')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_year_dict[0]"
      ],
      "metadata": {
        "id": "2E5kGnlfMSw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bGdB_6Q8tKf"
      },
      "source": [
        "for k in tqdm(token_year_dict.keys()):\n",
        "    DTM.add_doc(token_year_dict[k]['allowed_tokens'],token_year_dict[k]['publication_year'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wT3HpL_n84TH"
      },
      "source": [
        "##### 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DTM.train(0)"
      ],
      "metadata": {
        "id": "au4F4AYuM1Os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtJo3tzC9blM"
      },
      "source": [
        "# 한번에 20회씩 총 500회 학습\n",
        "print('Num docs:', len(DTM.docs), ', Vocab size:', DTM.num_vocabs, ', Num words:', DTM.num_words)\n",
        "print('Removed top words:', DTM.removed_top_words)\n",
        "print('Training...', file=sys.stderr, flush=True)\n",
        "for i in range(0, 500, 20):\n",
        "    DTM.train(20)\n",
        "    print('Iteration: {}\\tLog-likelihood: {}'.format(i, DTM.ll_per_word))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EF-ThDrz9epC"
      },
      "source": [
        "# 학습 결과\n",
        "DTM.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkTYhtS9-OMd"
      },
      "source": [
        "##### 토픽별 TopN 단어 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwV6jqX4-Q9R"
      },
      "source": [
        "for i in range(DTM.k):\n",
        "    for t in range(3):\n",
        "        res = DTM.get_topic_words(i,t,top_n=10)\n",
        "        print('Topic #{}'.format(i), end='\\t')\n",
        "        print(', '.join(w for w, p in res))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVTWqoG8-atS"
      },
      "source": [
        "##### 토픽 이름 자동으로 붙이기(참고용)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSUFnGCz-fTv"
      },
      "source": [
        "# extract candidates for auto topic labeling\n",
        "extractor = tp.label.PMIExtractor(min_cf=10, min_df=10, max_len=5, max_cand=10000)\n",
        "cands = extractor.extract(DTM)\n",
        "\n",
        "labeler = tp.label.FoRelevance(DTM, cands, min_df=10, smoothing=1e-2, mu=0.25)\n",
        "for k in range(DTM.k):\n",
        "    print(\"== Topic #{} ==\".format(k))\n",
        "    print(\"Labels:\", ', '.join(label for label, score in labeler.get_topic_labels(k, top_n=5)))\n",
        "    for t in range(3):\n",
        "        for word, prob in DTM.get_topic_words(k, t,top_n=10):\n",
        "            print(word, prob, sep='\\t')\n",
        "        print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSwkjffN_KXo"
      },
      "source": [
        "##### 시간에 따른 토픽 비중 변화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DP8VkT79-9rU"
      },
      "source": [
        "topic_dist_by_time = np.zeros(shape=[DTM.num_timepoints, DTM.k], dtype=np.float64)\n",
        "for doc in DTM.docs:\n",
        "    topic_dist_by_time[doc.timepoint] += doc.get_topic_dist()\n",
        "\n",
        "topic_dist_by_time /= DTM.num_docs_by_timepoint[:, np.newaxis]\n",
        "\n",
        "for k in range(DTM.k):\n",
        "    print('Topic #{}'.format(k), *(w for w, _ in DTM.get_topic_words(k, 0, top_n=5)))\n",
        "    print(topic_dist_by_time[:, k])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQzHY18F_Z5N"
      },
      "source": [
        "topic_dist_by_time = pd.DataFrame(topic_dist_by_time)\n",
        "topic_dist_by_time.index = [2022,2023,2024]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pS57fo5cATXb"
      },
      "source": [
        "f = plt.figure()\n",
        "plt.title('Topic distribution by year', color='black')\n",
        "topic_dist_by_time.plot(ax=f.gca(),colormap='gnuplot')\n",
        "plt.xticks(np.array(range(2022,2025)),rotation=45)\n",
        "ax=f.gca()\n",
        "ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
        "ax.set_ylabel('Percent')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Trk7UKRXs8Sh"
      },
      "source": [
        "### 3. DMR (Dirichlet Multinomial Regression)\n",
        "\"Topic proportion differences by categorical variables\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkDsJQwo-pav"
      },
      "source": [
        "##### setting up categorical variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9q5sdG5LtB8l"
      },
      "source": [
        "df.loc[df['field_name'].str.len()>0,'field_name'].map(lambda x:x[0]).value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzybMINZ-wa1"
      },
      "source": [
        "# Top 4 field_name\n",
        "top4_field = df.loc[df['field_name'].str.len()>0,'field_name'].map(lambda x:x[0]).value_counts().index[:4].tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQCk2hkfDYnj"
      },
      "source": [
        "df_field = df.loc[df['field_name'].str.len()>0].reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_field['field_name'] = df_field['field_name'].map(lambda x:x[0])"
      ],
      "metadata": {
        "id": "BkkIG0b6RKBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: top4_field가 아니면 other로\n",
        "df_field['field_name_top4'] = df_field['field_name'].apply(lambda x: x if x in top4_field else 'other')"
      ],
      "metadata": {
        "id": "WCjbPUCvRbSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03VGq6AMDgNn"
      },
      "source": [
        "##### 모델 초기화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sjQiEzgDhjl"
      },
      "source": [
        "DMR = tp.DMRModel(k=10,min_df=10,tw=tp.TermWeight.PMI,rm_top=3, seed=2021)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NA6-CCT-DkTJ"
      },
      "source": [
        "##### 빈 모델에 토큰 리스트와 명목변수 넣어주기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uQ9-nhYDq_v"
      },
      "source": [
        "token_cate_dict = df_field[['allowed_tokens','field_name_top4']].to_dict('index')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3AcUirFEVdL"
      },
      "source": [
        "for k in tqdm(token_cate_dict.keys()):\n",
        "    DMR.add_doc(token_cate_dict[k]['allowed_tokens'],token_cate_dict[k]['field_name_top4'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lliRKItwD3Ff"
      },
      "source": [
        "##### 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYSZyrEuEZpx"
      },
      "source": [
        "DMR.train(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFEuhxvtEaQd"
      },
      "source": [
        "# 한번에 20회씩 총 500회 학습\n",
        "print('Num docs:', len(DMR.docs), ', Vocab size:', DMR.num_vocabs, ', Num words:', DMR.num_words)\n",
        "print('Removed top words:', DMR.removed_top_words)\n",
        "print('Training...', file=sys.stderr, flush=True)\n",
        "for i in range(0, 500, 20):\n",
        "    DMR.train(20)\n",
        "    print('Iteration: {}\\tLog-likelihood: {}'.format(i, DMR.ll_per_word))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_s7WB26D49M"
      },
      "source": [
        "##### 토픽별 TopN 단어 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7PfuBhbD-Ae"
      },
      "source": [
        "for i in range(DMR.k):\n",
        "    res = DMR.get_topic_words(i, top_n=10)\n",
        "    print('Topic #{}'.format(i), end='\\t')\n",
        "    print(', '.join(w for w, p in res))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuXuptBuD_pN"
      },
      "source": [
        "##### 토픽 이름 자동으로 붙이기 (참고용)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ii5SwhfUED1K"
      },
      "source": [
        "# extract candidates for auto topic labeling\n",
        "extractor = tp.label.PMIExtractor(min_cf=10, min_df=10, max_len=5, max_cand=10000)\n",
        "cands = extractor.extract(DMR)\n",
        "\n",
        "labeler = tp.label.FoRelevance(DMR, cands, min_df=10, smoothing=1e-2, mu=0.25)\n",
        "for k in range(DMR.k):\n",
        "    print(\"== Topic #{} ==\".format(k))\n",
        "    print(\"Labels:\", ', '.join(label for label, score in labeler.get_topic_labels(k, top_n=5)))\n",
        "    for word, prob in DMR.get_topic_words(k, top_n=10):\n",
        "        print(word, prob, sep='\\t')\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVbi9WNMFICG"
      },
      "source": [
        "##### metadata에 따른 토픽 분포 대조\n",
        "https://github.com/bab2min/tomotopy/blob/main/examples/dmr_plot.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R72l7afcFYD1"
      },
      "source": [
        "DMR.metadata_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WJPNlVEFZPU"
      },
      "source": [
        "# calculate topic distribution for each metadata using softmax\n",
        "probs = np.exp(DMR.lambdas - DMR.lambdas.max(axis=0))\n",
        "probs /= probs.sum(axis=0)\n",
        "\n",
        "print('Topic proportions by categorical variable')\n",
        "for f, metadata_name in enumerate(DMR.metadata_dict):\n",
        "    print(metadata_name, probs[:, f], '\\n')\n",
        "\n",
        "x = np.arange(DMR.k)\n",
        "width = 1 / (DMR.f + 2)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))  # Increase figure size for better visibility\n",
        "for f, metadata_name in enumerate(DMR.metadata_dict):\n",
        "    ax.bar(x + width * (f - DMR.f / 2), probs[:, f], width, label=DMR.metadata_dict[f])\n",
        "\n",
        "ax.set_ylabel('Probabilities')\n",
        "ax.set_yscale('log')\n",
        "ax.set_title('Topic proportions by categorical variable')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(['Topic #{}'.format(k) for k in range(DMR.k)], rotation=45)  # Rotate x-axis labels by 45 degrees\n",
        "\n",
        "# Move the legend outside of the plot\n",
        "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "# Adjust subplot parameters to give more room to the legend\n",
        "fig.subplots_adjust(right=0.75)  # Adjust the right space to accommodate the legend\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}